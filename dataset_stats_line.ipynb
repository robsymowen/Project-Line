{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73c3777-77a5-4707-8c5a-91109fce6fc0",
   "metadata": {},
   "source": [
    "# compute stats for line drawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6abc5e6e-99ae-4448-b6ee-65f4b597153e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.449, 0.226)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array([0.485, 0.456, 0.406]).mean(), np.array([0.229, 0.224, 0.225]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d11f8e8-cb19-4aa0-8e93-4b6bcfceb095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import ImageNet\n",
    "from torchvision import transforms\n",
    "\n",
    "class InvertTensorImageColors(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InvertTensorImageColors, self).__init__()\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Check if the image is in uint8 format\n",
    "        if img.dtype == torch.uint8:\n",
    "            return 255 - img\n",
    "        # assume the image is in float format\n",
    "        else:\n",
    "            return 1 - img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c288a24d-a493-4715-aa70-f3e4dda589d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from fastprogress import progress_bar\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "    \n",
    "def cv2_loader(path, to_rgb=True):\n",
    "    img = cv2.imread(path)\n",
    "    if to_rgb: img = img[:,:,::-1]\n",
    "    \n",
    "    return img\n",
    "\n",
    "def load_image(p, to_rgb=True):\n",
    "    '''Our default image loader, takes `filename` and returns a PIL Image. \n",
    "        Speedwise, turbo_loader > pil_loader > cv2, but cv2 is the most robust, so \n",
    "        we try to load jpg images with turbo_loader, fall back to PIL, then cv2.\n",
    "        \n",
    "        This fallback behavior is needed, e.g., with ImageNet there are a few images\n",
    "        that either aren't JPEGs or have issues that turbo_loader crashes on, but cv2 \n",
    "        doesn't.\n",
    "    '''\n",
    "    if p.lower().endswith('.jpg') or p.lower().endswith('.jpeg'): \n",
    "        try:\n",
    "            img = pil_loader(p)\n",
    "        except:\n",
    "            img = cv2.imread(p)\n",
    "            if to_rgb: img = img[:,:,::-1]\n",
    "    else:\n",
    "        try:\n",
    "            img = pil_loader(p)\n",
    "        except:\n",
    "            img = cv2.imread(p)\n",
    "            if to_rgb: img = img[:,:,::-1]\n",
    "                \n",
    "    if img is not None and not isinstance(img, Image.Image):\n",
    "        img = Image.fromarray(img)\n",
    "        \n",
    "    return img\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate(batch):\n",
    "    # Extract images and labels\n",
    "    images, labels = zip(*batch)\n",
    "    return images, torch.tensor(labels)\n",
    "\n",
    "def get_dataloader(dataset, batch_size=256, prefetch_factor=10,\n",
    "                   num_workers=len(os.sched_getaffinity(0))):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers, \n",
    "                            shuffle=False, pin_memory=True, prefetch_factor=prefetch_factor,\n",
    "                            collate_fn=custom_collate)\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52b04c0-b244-4a4f-8938-dca0c4dd586b",
   "metadata": {},
   "source": [
    "# compute line-drawing stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8d9f22-8a9a-4c27-be53-4658ad483c1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageNet\n",
       "    Number of datapoints: 1281167\n",
       "    Root location: /n/alvarez_lab_tier1/Users/alvarez/datasets/imagenet1k-line/imagenet1k-anime_style\n",
       "    Split: train\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
       "               ToTensor()\n",
       "               InvertTensorImageColors()\n",
       "           )"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "style_name = \"anime_style\"\n",
    "split = \"train\"\n",
    "root_directory = os.path.join(os.environ['SHARED_DATA_DIR'], 'imagenet1k-line', f\"imagenet1k-{style_name}\")\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.ToTensor(),\n",
    "    InvertTensorImageColors(),\n",
    "])\n",
    "dataset = ImageNet(root_directory, split=split, transform=transform)\n",
    "assert len(dataset)==1281167, f\"Expected 1281167 val images, got {len(dataset)}\"\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6201b3d-6f6c-4a04-b37a-3f23f081efad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageNet\n",
      "    Number of datapoints: 1281167\n",
      "    Root location: /n/alvarez_lab_tier1/Users/alvarez/datasets/imagenet1k-line/imagenet1k-anime_style\n",
      "    Split: train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Resize(size=256, interpolation=bilinear, max_size=None, antialias=None)\n",
      "               ToTensor()\n",
      "               InvertTensorImageColors()\n",
      "           )\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1488c97ea8b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = get_dataloader(dataset, batch_size=256, prefetch_factor=2)\n",
    "print(dataloader.dataset)\n",
    "dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25e25aeb-0e5d-493a-9e9a-8813ee9ad4f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='5005' class='' max='5005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [5005/5005 1:50:54&lt;00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean torch.Size([1281167, 3])\n",
      "std torch.Size([1281167, 3])\n",
      "mean torch.Size([1281167, 3])\n",
      "std torch.Size([1281167, 3])\n"
     ]
    }
   ],
   "source": [
    "from pdb import set_trace\n",
    "from collections import defaultdict\n",
    "\n",
    "line_stats = defaultdict(list)\n",
    "line_stats_nonzero = defaultdict(list)\n",
    "for batch_idx,(imgs,labels) in enumerate(progress_bar(dataloader)):\n",
    "    for img in imgs:\n",
    "        img_flat = img.flatten(start_dim=-2) # flattened over space\n",
    "        \n",
    "        # including zeros\n",
    "        img_mean = img_flat.mean(dim=-1) # averaged over space\n",
    "        img_std = img_flat.std(dim=-1) # std over space\n",
    "        line_stats['mean'].append(img_mean) \n",
    "        line_stats['std'].append(img_std)   \n",
    "        \n",
    "        # excluding zeros\n",
    "        non_zero_mask = img_flat != 0\n",
    "        \n",
    "        # Initialize a tensor to store the means and stds\n",
    "        non_zero_means = torch.zeros(3)\n",
    "        non_zero_stds = torch.zeros(3)\n",
    "        \n",
    "        # Compute mean for each channel\n",
    "        for channel_idx in range(3):\n",
    "            # Select non-zero elements for the current channel\n",
    "            non_zero_elements = img_flat[channel_idx][non_zero_mask[channel_idx]]\n",
    "\n",
    "            # Compute the mean of these elements\n",
    "            if non_zero_elements.nelement() != 0:  # Check to avoid division by zero\n",
    "                non_zero_means[channel_idx] = non_zero_elements.mean()\n",
    "                non_zero_stds[channel_idx] = non_zero_elements.std()\n",
    "\n",
    "        line_stats_nonzero['mean'].append(non_zero_means) \n",
    "        line_stats_nonzero['std'].append(non_zero_stds)\n",
    "    \n",
    "for k,v in line_stats.items():\n",
    "    line_stats[k] = torch.stack(line_stats[k])\n",
    "    print(k, line_stats[k].shape)\n",
    "    \n",
    "for k,v in line_stats_nonzero.items():\n",
    "    line_stats_nonzero[k] = torch.stack(line_stats_nonzero[k])\n",
    "    print(k, line_stats_nonzero[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6331aada-2e3f-4483-9981-217a165bbc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([0.0676, 0.0676, 0.0676])\n",
      "std tensor([0.1459, 0.1459, 0.1459])\n"
     ]
    }
   ],
   "source": [
    "for k,v in line_stats.items():\n",
    "    print(k, line_stats[k].mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97e484a7-9b97-48c3-ae7d-a88a65478fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean tensor([0.1241, 0.1241, 0.1241])\n",
      "std tensor([0.1814, 0.1814, 0.1814])\n"
     ]
    }
   ],
   "source": [
    "for k,v in line_stats_nonzero.items():\n",
    "    print(k, line_stats_nonzero[k].mean(dim=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5840040d-6cb0-4a19-a5cb-0fb97049b49a",
   "metadata": {},
   "source": [
    "# line stats\n",
    "\n",
    "line drawing stats (including zeros):\n",
    "- mean [0.0676, 0.0676, 0.0676]\n",
    "- std [0.1459, 0.1459, 0.1459]\n",
    "\n",
    "line drawing stats (excluding zeros):\n",
    "- mean [0.1241, 0.1241, 0.1241]\n",
    "- std [0.1814, 0.1814, 0.1814]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15ba6da2-38fb-437c-9f84-c810d0f157f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68412348, 0.68412348, 0.68412348])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array([0.1241, 0.1241, 0.1241])/0.1814"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50362188-dcad-426e-b646-d62f60756087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54911504, 0.54911504, 0.54911504])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([0.1241, 0.1241, 0.1241])/np.mean([0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fcfe9-152a-44fd-ad2c-bcb4d9136194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
