# -*- coding: utf-8 -*-
"""Cluster_LineDrawing_Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/robsymowen/Project-Line/blob/main/Cluster_LineDrawing_Analysis.ipynb
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn.modules.container import ModuleList
import torch.optim as optim
from torch.optim.lr_scheduler import _LRScheduler
from torchvision.models import alexnet
import torch.utils.data as data

import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torchvision import models

import tarfile


from sklearn import decomposition
from sklearn import manifold
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from tqdm.notebook import tqdm, trange
import matplotlib.pyplot as plt
import numpy as np
from fastprogress import master_bar, progress_bar

from scipy.special import modfresnelm


import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

import copy
import os
import random
import time
import hashlib
import shutil
import glob

import argparse
from glob import glob
import os


# Create the parser
parser = argparse.ArgumentParser(description='Process line drawing conversion arguments.')
# Add arguments
parser.add_argument('line_tarred_dir', type=str, help='The root directory of the ImageNet dataset')
parser.add_argument('rgb_tarred_dir', type=str, help='The directory where output will be stored')
parser.add_argument('rgb_checkpoint_path', type=str, help='The root directory of the ImageNet dataset')
parser.add_argument('line_checkpoint_path', type=str, help='The directory where output will be stored')

# Parse the arguments
args = parser.parse_args()

# Use the parsed arguments

# Directories to our tarred datasets
line_tarred_dir = args.line_tarred_dir
rgb_tarred_dir = args.rgb_tarred_dir

if torch.cuda.is_available():
    device = torch.device('cuda')
    print('CUDA is available. Using GPU.')
else:
    device = torch.device('cpu')
    print('CUDA is not available. Using CPU')

"""#Load Models"""

def load_model(checkpoint_path):
    checkpoint = torch.load(checkpoint_path)

    # Load a pre-trained AlexNet model or create a new one
    model = alexnet(pretrained=False)  # Set to False if you don't want to use pre-trained weights

    # If you're training from scratch, you might want to modify the last layer to fit your number of classes
    num_classes = 10  # replace with your number of classes
    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)

    # Load in weights
    model.load_state_dict(checkpoint['state_dict'])

    # Move model to GPU if available
    model = model.to(device)

    return model

# Load Models

rgb_model = load_model(rgb_checkpoint_path)
line_model = load_model(line_checkpoint_path)
print(line_model.features)

"""# Load Datasets from Drive to Local Directory"""

## Helpers for hashing filename and checking hash

# Helper for calculating hash id for any file path
def get_hash(filename):
    sha256 = hashlib.sha256()
    with open(filename, 'rb') as file:
        for chunk in iter(lambda: file.read(8192), b''):
            sha256.update(chunk)

    # Get the last 10 digits of the hash
    hash_id = sha256.hexdigest()[-10:]
    return hash_id

#  helper - check hash ID
def check_hash_id(filename):
    # Extract the expected hash value from the filename
    expected_hash = filename.split('-')[-1].split('.')[0]

    if not (os.path.exists(filename) and os.path.isfile(filename)):
        raise FileNotFoundError(f"The file {filename} does not exist or the path is incorrect.")

    hash_id = get_hash(filename)

    # Check if the end of the file name matches the hash_id
    if expected_hash != hash_id:
        raise ValueError(f"The end of the file name {expected_hash} does not match the calculated hash_id {hash_id}.")

def load_dataset(drive_tarred_dir):
    tarred_dataset_name = drive_tarred_dir.split('/')[-1]

    # Create content/data directory, for holding datasets
    os.makedirs('/content/data', exist_ok=True)

    # Move dataset to content/data
    dst_path ='/content/data/' + tarred_dataset_name
    shutil.copy(drive_tarred_dir, dst_path)

    # Check that the dataset is correctly hashed
    check_hash_id(dst_path)
    print("Hash is correct")

    # Untar Dataset
    with tarfile.open(dst_path, 'r') as tar:
        tar.extractall('/content/data/')

    dataset_name = tarred_dataset_name.split('-' + get_hash(dst_path))[0]

    content_dir_path = '/content/data/' + dataset_name

    return content_dir_path

# Load Datasets and set directories in the local /content/data directory
line_dir = load_dataset(line_tarred_dir)
rgb_dir = load_dataset(rgb_tarred_dir)

line_val_dir = line_dir + '/val'
rgb_val_dir = rgb_dir + '/val'

print(line_val_dir)

# Initialize and transform Validation Dataset:

# Load your dataset
def create_dataloader(valdir):

    # Assuming your data is structured in a directory where each sub-directory represents a class
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                        std=[0.229, 0.224, 0.225])

    folder = datasets.ImageFolder(
                valdir,
                transforms.Compose([
                    transforms.Resize(256),
                    transforms.CenterCrop(224),
                    transforms.ToTensor(),
                    normalize,
                ]))

    loader = torch.utils.data.DataLoader(folder, batch_size=256, shuffle=False)
    return loader

line_val_dataloader = create_dataloader(line_val_dir)
rgb_val_dataloader = create_dataloader(rgb_val_dir)


image_files = glob.glob(os.path.join(line_dir+'/train', "**","**", "*.png"))
num_images = len(image_files)
print(num_images)

"""#Compare Model Performance"""

from fastprogress import master_bar, progress_bar

def test_alexnet(model, val_loader, device):
    print("Testing model ")
    # Set model to eval mode
    model.eval()

    correct = 0
    total = 0

    with torch.no_grad():

        for images, labels in progress_bar(val_loader):
            images, labels = images.to(device), labels.to(device)

            # Run model on images and create predictions from output layer.
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)

            # Update accumulators
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    # Compute and return accuracy as percentage:
    accuracy = 100 * correct / total
    return accuracy

# Test both models on their respective validation sets
accuracy_results = {'model': [], 'line_val_acc': [], 'rgb_val_acc': []}

def append_results(model, model_name, device):
    line_accuracy = test_alexnet(model, line_val_dataloader, device)
    rgb_accuracy = test_alexnet(model, rgb_val_dataloader, device)

    accuracy_results['model'].append(model_name)
    accuracy_results['line_val_acc'].append(line_accuracy)
    accuracy_results['rgb_val_acc'].append(rgb_accuracy)

append_results(rgb_model, 'rgb model', device)
append_results(line_model, 'line model', device)

# Display results
results_df = pd.DataFrame(accuracy_results)
print(results_df)

fig, axs = plt.subplots(1, 2, figsize=(10,5))

sns.barplot(data=results_df, x='model', y='line_val_acc',
            ax=axs[0], color='b', )
axs[0].set_title('Line Drawing Validation Set Accuracy')

sns.barplot(data=results_df, x='model', y='rgb_val_acc',
            ax=axs[1], color='r')
axs[1].set_title('RGB Imagenette Validation Set Accuracy')

axs[0].set_ylim(0, 100)
axs[1].set_ylim(0, 100)

plt.show();

"""#Look at Convolutional Kernels

"""

import torch.nn as nn
from torchvision.utils import make_grid
import matplotlib.pyplot as plt


# Display convolutional kernels of each model
def show_conv1(model, nrow=16):
    # find first conv
    first_conv = None
    for m in model.modules():
        if isinstance(m, nn.Conv2d):
            first_conv = m
            break

    if first_conv is not None:
        kernels = first_conv.weight.detach().clone().cpu()
        kernels = kernels - kernels.min()
        kernels = kernels / kernels.max()
        img = make_grid(kernels, nrow=nrow)
        #img = img[0]
        print(img.shape)
        plt.imshow(img.permute(1, 2, 0))
    else:
        print("failed to find first conv layer")

print('Convolutional Kernels of Model Trained on Line Drawing:')
show_conv1(line_model)

print('Convolutional Kernels of Model Trained on Imagenette:')
show_conv1(rgb_model)

"""#Create Confusion Matrices"""

y_true = []
y_pred = []

from fastprogress import master_bar, progress_bar

def create_confusion_matrix(model, val_loader, device):
    print("Generating Predictions")
    # Set model to eval mode
    model.eval()

    y_true = []
    y_pred = []

    with torch.no_grad():

      for images, labels in progress_bar(val_loader):
          images, labels = images.to(device), labels.to(device)

          labels = labels.detach().cpu().numpy()

          y_true.extend(labels.flatten())

          # Run model on images and create predictions from output layer.
          outputs = model(images)
          _, predicted = torch.max(outputs.data, 1)

          predicted = predicted.detach().cpu().numpy()

          y_pred.extend(predicted.flatten())

      matrix = confusion_matrix(y_true, y_pred)

    # Compute and return accuracy as percentage:
    return matrix

torch.cuda.empty_cache()

line_conf_matrix = create_confusion_matrix(line_model, rgb_val_dataloader, device)
rgb_conf_matrix = create_confusion_matrix(rgb_model, rgb_val_dataloader, device)

fig, axs = plt.subplots(2,1, figsize=(10,10))

sns.heatmap(line_conf_matrix, ax=axs[0])
axs[0].set_title('Line Drawing Trained Model Confusion Matrix')
sns.heatmap(rgb_conf_matrix, ax=axs[1])
axs[1].set_title('RGB Trained Model Confusion Matrix')

plt.show();

print("Line Drawing Matrix: \n", line_conf_matrix)
print("RGB Matrix: \n", line_conf_matrix)

# Get the list of subfolders (class labels)
class_labels = [label for label in os.listdir(line_val_dir) if os.path.isdir(os.path.join(line_val_dir, label))]

# Print the class labels
print("Class Labels:", class_labels)

folder = datasets.ImageFolder(
            line_val_dir,
            transforms.Compose([
                transforms.Resize(256),
                transforms.CenterCrop(224),
                transforms.ToTensor()
            ]))

folder.class_to_idx

"""
#Compare Activations"""

# Helper to be used as hook, for extracting activations at each layer
def get_activation(activations, name):
    def hook(model, input, output):
        activations[name].append(output.detach().flatten())
    return hook

# Helper for storing name of layer in a constant way (so it's always the same)
def get_layer_name(i, layers, features=True):
    if features:
        layer_name = str(i) + '_' + str(layers[i]).split('(')[0]
    else:
        layer_name = str(i+12) + '_' + str(layers[i]).split('(')[0]
    return layer_name

# Checks for existing hooks, adds them to layers if not already present
def create_hooks(model, activations, features=True, classifier=True):

    if has_hooks(model):
        return 1

    else:

        model_children = list(model.children())

        # Create list of layers to iterate through
        layers = []
        if features:
            layers = list(model_children[0])
        if classifier:
            layers += list(model_children[2])



        for i in range(len(layers)):
            layer_name = get_layer_name(i, layers, features=features)
            if i < 13:
                model_children[0][i].register_forward_hook(get_activation(activations, layer_name))
            else:
                model_children[2][i-13].register_forward_hook(get_activation(activations, layer_name))

        return 0


def has_hooks(model):
    return len(model._forward_hooks) > 0

# Initializes dict of activations- dict where keys are layers of the model

def init_activations(model):
  # Initialize activations dict of activation matrices:
  activations = {}
  model_children = list(model.children())

  # Create list of layers to iterate through
  layers = list(model_children[0])+list(model_children[2])

  # Set a key for each layer pointing to empty list
  for i in range(len(layers)):
    layer_name = get_layer_name(i, layers)
    activations[layer_name] = []

  return activations

from torch.autograd import Variable
from PIL import Image

# Create Feature Matrix for a given layer (each row is an image)
def extract_activations(model, img_path, device, activations):

    # Transforms to apply to image
    preprocess = transforms.Compose([
    transforms.Resize(256),
    transforms.CenterCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

    # Load image from path
    img = Image.open(img_path)

    # Check for format of image, reformat if needed
    if img.mode != 'RGB':
        img = img.convert('RGB')

    # Apply transforms
    img_tensor = preprocess(img).unsqueeze(0).to(device)

    with torch.no_grad():
        # Get activations
        model.eval()
        output = model(img_tensor)

    return activations

# Create Feature/Activation Matrix
def create_activation_matrices(model, val_dir, device):

    model.eval()

    # Initialize activations array
    activation_matrix = init_activations(model)

    # Create hooks on the model
    create_hooks(model, activation_matrix, features=True, classifier=True)

    # Glob images into one list of images
    # Should somehow make sure these are sorted - this would guarantee the same order - call sorted()
    image_files = glob.glob(os.path.join(val_dir, "**","**", "*.png"))
    if len(image_files) < 3000:
        image_files = glob.glob(os.path.join(val_dir, "**", "*.JPEG"))

    # Trim your number of images (to run faster)
    image_files = np.sort(image_files[::50])
    image_files = image_files[0:78]

    print("Creating Activation Matrix")
    for img in progress_bar(image_files):
        # Extract activations and append to matrix
        activation_matrix = extract_activations(model, img, device, activation_matrix)

    return activation_matrix

from scipy.stats import pearsonr

# Representational Similarity Matrix, using activations from all layers
def create_RSM(activation_matrix):

    N = len(activation_matrix)

    # Create blank RSM that is N X N (N = # Images)
    similarity_matrix = np.zeros(shape=(N,N))

    # Pairwise similarity between all images
    print("Computing Pearson Similiarities")
    for i in progress_bar(range(N)):
        for j in range(i+1, N):
            pearson_coefficient, p_value = pearsonr(activation_matrix[i].cpu(), activation_matrix[j].cpu())
            similarity_matrix[i][j] = pearson_coefficient

    return similarity_matrix

# Compare upper triangles of two models, produce pearson similarity
def compare_model_activations(models, val_dirs, device, results):

    # Create activation Matrices
    activation_matrices_1 = create_activation_matrices(models[0], val_dirs[0], device)
    activation_matrices_2 = create_activation_matrices(models[1], val_dirs[1], device)

    # Select the layers you want to compare -- should be the post-RELU activations
    layers_to_compare = [1, 2, 4, 5, 7, 8, 9, 11, 12, 15, 18, 19]

    # Iterate through layers

    layer_idx = 0

    for layer in activation_matrices_1.keys():
        if layer_idx in layers_to_compare:

            print("\nLayer: ", layer)

            # Create Representational Similarity Matrices for each Model
            # np.triu instead of flatten (since we are including zeros here)

            rsm_1 = create_RSM(activation_matrices_1[layer])
            rsm_1_vector = rsm_1[np.triu_indices(rsm_1.shape[0], k=1)]

            rsm_2 = create_RSM(activation_matrices_2[layer])
            rsm_2_vector = rsm_2[np.triu_indices(rsm_2.shape[0], k=1)]

            # Pearson simliarities of upper triangle vectors
            similarity, _ = pearsonr(rsm_1_vector, rsm_2_vector)

            # Add to dict of similarities
            results['layer'].append(layer)

            results['pearson_similarity'].append(similarity)
            results['layer_idx'].append(layer_idx)

            # Plot df of similarities for each layer
            results_df = pd.DataFrame(results)

            # Show graph
            plt.figure(figsize=(10,5))

            ax = sns.lineplot(data=results_df, x='layer', y='pearson_similarity')
            plt.title("Pearson Similarity of Line Drawing and RGB-trained Alexnets")

            plt.show();


            print(results_df)

        layer_idx += 1


    print(results_df)


    return results_df

rgb_results = {'layer': [],'layer_idx':[], 'pearson_similarity': []}

# Call function to compare activations (computes activations, pearson similarities)
compare_model_activations([line_model, rgb_model],
                          [line_val_dir, rgb_val_dir],
                          device, rgb_results)

# Format results and graph them one last time
# results_df = pd.DataFrame(results)
results_df['rgb_rgb'] = rgb_results['pearson_similarity']

plt.figure(figsize=(15,5))


ax = sns.lineplot(data=results_df, x='layer', y='pearson_similarity')
ax = sns.lineplot(data=results_df, x='layer', y='rgb_rgb')

plt.legend()

plt.title("Pearson Similarity of Line Drawing and RGB-trained Alexnets")

results_df
